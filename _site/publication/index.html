<!DOCTYPE html>
<html>
<head>
  <link rel="icon" type="image/png" href="/favicon.png">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Haofei Yu | Publication</title>
  <meta name="description" content="Personal website of Haofei Yu.">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="http://localhost:4000/assets/css/all.min.css">
  <link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.min.css">
  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="canonical" href="http://localhost:4000/publication/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/"><span class="font-weight-bold">Haofei</span> Yu</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
            <li class="nav-item ">
                <a class="nav-link" href="http://localhost:4000/">
                About
                
                </a>
            </li>

            <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/cv.pdf">
                    CV
                </a>
            </li>


            
                
                    
                    
                
            
                
            
                
                    
                        
                
                    
                        
                
                    
                        
                
            

        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Publication</h1>
  <h6><nobr><em>*</em></nobr> denotes equal contribution and joint lead authorship.</h6>


<p><br /></p>

<div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
    <div class="col-sm-1 mt-2 p-0 pr-1">
      <h3 class="bibliography-year">2023</h3>
    </div>
    <div class="col-sm-11 p-0">
      <ol class="bibliography"><li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-10 p-0 iabbr">
        
        
        <span class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="white-space: normal; overflow: hidden; text-overflow: ellipsis;">
            preprint
        </span>
        
        
    </div>
  <div class="col-sm-11 mt-0 mt-sm-0 p-0">
    
    <div id="zhou2023sotopia" class="col p-0">
      <h5 class="title mb-0">MMOE: Mixture of Multimodal Interaction Experts.</h5>
      <div class="author">
        
          
            
              
                <nobr><em><nobr><em>Haofei</em></nobr> <nobr><em>Yu</em></nobr></em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Paul Pu Liang,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Ruslan Salakhutdinov,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr>Louis-Philippe Morency</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In arXiv
          
          
            2023.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#zhou2023sotopia-abstract" role="button" aria-expanded="false" aria-controls="zhou2023sotopia-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2311.09580" target="_blank">PDF</a>
        
        
        
        
        
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="zhou2023sotopia-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. However, this is just a portion of the potential multimodal interactions seen in the real world and does not include new interactions between conflicting utterances and gestures in predicting sarcasm, for example. Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts. Our method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction. Based on our experiments, this approach improves performance on these challenging interactions by more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, interaction quantification provides new insights for dataset analysis and yields simple approaches that obtain state-of-the-art performance.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-10 p-0 iabbr">
        
        
        <span class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="white-space: normal; overflow: hidden; text-overflow: ellipsis;">
            preprint
        </span>
        
        
    </div>
  <div class="col-sm-11 mt-0 mt-sm-0 p-0">
    
    <div id="yu2023mmoe" class="col p-0">
      <h5 class="title mb-0">SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr>Xuhui Zhou*,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Hao Zhu*,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Leena Mathur,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Ruohong Zhang,</nobr>
                
              
            
          
        
          
            
              
                <nobr><em><nobr><em>Haofei</em></nobr> <nobr><em>Yu</em></nobr></em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Zhengyang Qi,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Louis-Philippe Morency,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Yonatan Bisk,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Daniel Fried,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Graham Neubig,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr> others</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In arXiv
          
          
            2023.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#yu2023mmoe-abstract" role="button" aria-expanded="false" aria-controls="yu2023mmoe-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2310.11667" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/sotopia-lab/sotopia" target="_blank">Code</a>
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="yu2023mmoe-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systemsâ€™ abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIAâ€™s promise as a general platform for research on evaluating and improving social intelligence in artificial agents.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-10 p-0 iabbr">
        
        
        <span class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="white-space: normal; overflow: hidden; text-overflow: ellipsis;">
            EMNLP Findings 2023
        </span>
        
        
    </div>
  <div class="col-sm-11 mt-0 mt-sm-0 p-0">
    
    <div id="yu2023trams" class="col p-0">
      <h5 class="title mb-0">TRAMS: Training-free Memory Selection for Long-range Language Modeling.</h5>
      <div class="author">
        
          
            
              
                <nobr><em><nobr><em>Haofei</em></nobr> <nobr><em>Yu*</em></nobr></em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Cunxiang Wang*,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Yue Zhang,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr>Wei Bi</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Findings of EMNLP
          
          
            2023.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#yu2023trams-abstract" role="button" aria-expanded="false" aria-controls="yu2023trams-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2310.15494" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/lwaekfjlk/TRAMS" target="_blank">Code</a>
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="yu2023trams-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-10 p-0 iabbr">
        
        
        <span class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="white-space: normal; overflow: hidden; text-overflow: ellipsis;">
            EMNLP 2023
        </span>
        
        
    </div>
  <div class="col-sm-11 mt-0 mt-sm-0 p-0">
    
    <div id="weissweiler2023counting" class="col p-0">
      <h5 class="title mb-0">Counting the Bugs in ChatGPTâ€™s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr>Leonie Weissweiler*,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Valentin Hofmann*,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Anjali Kantharuban,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Anna Cai,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Ritam Dutt,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Amey Hengle,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Anubha Kabra,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Atharva Kulkarni,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Abhishek Vijayakumar,</nobr>
                
              
            
          
        
          
            
              
                <nobr><em><nobr><em>Haofei</em></nobr> <nobr><em>Yu</em></nobr></em>,</nobr>
              
            
          
        
          
            
              and
              
                
                  <nobr> others</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In EMNLP
          
          
            2023.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#weissweiler2023counting-abstract" role="button" aria-expanded="false" aria-controls="weissweiler2023counting-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2310.15113" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/dmort27/chatgpts-wugs" target="_blank">Code</a>
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="weissweiler2023counting-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berkoâ€™s (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results â€“ through the lens of morphology â€“ cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-10 p-0 iabbr">
        
        
        <span class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="white-space: normal; overflow: hidden; text-overflow: ellipsis;">
            ACL Findings 2023
        </span>
        
        
    </div>
  <div class="col-sm-11 mt-0 mt-sm-0 p-0">
    
    <div id="wang-etal-2023-rfid" class="col p-0">
      <h5 class="title mb-0">RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question Answering.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr>Cunxiang Wang*,</nobr>
                
              
            
          
        
          
            
              
                <nobr><em><nobr><em>Haofei</em></nobr> <nobr><em>Yu*</em></nobr></em>,</nobr>
              
            
          
        
          
            
              and
              
                
                  <nobr>Yue Zhang</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Findings of ACL
          
          
            2023.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#wang-etal-2023-rfid-abstract" role="button" aria-expanded="false" aria-controls="wang-etal-2023-rfid-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arxiv.org/abs/2305.17041" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/wangcunxiang/RFiD" target="_blank">Code</a>
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="wang-etal-2023-rfid-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Open-Domain Question Answering (ODQA) systems necessitate a reader model capable of generating answers by simultaneously referring to multiple passages. Although representative models like Fusion-in-Decoder (FiD) have been proposed to address this challenge, these systems can inadvertently rely on spurious features instead of genuine causal relationships between the question and the passages to generate answers. To counter this problem, we introduce the Rational Fusion-in-Decoder (RFiD) model. Our model leverages the encoders of FiD to differentiate between causal relationships and spurious features, subsequently guiding the decoder to generate answers informed by this discernment. Experimental results on two ODQA datasets, Natural Questions (NQ) and TriviaQA (TQ), demonstrate that our model surpasses previous methods, achieving improvements of up to 1.5 and 0.7 in Exact Match scores on NQ, and exhibits an enhanced ability to identify causal relationships.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li>
<li><div class="row m-0 mt-3 p-0">
    <div class="col-sm-10 p-0 iabbr">
        
        
        <span class="badge font-weight-bold danger-color-dark darken-1 align-middle" style="white-space: normal; overflow: hidden; text-overflow: ellipsis;">
            ACL Findings 2023
        </span>
        
        
    </div>
  <div class="col-sm-11 mt-0 mt-sm-0 p-0">
    
    <div id="song-etal-2023-uni" class="col p-0">
      <h5 class="title mb-0">Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems.</h5>
      <div class="author">
        
          
            
              
                
                  <nobr>Chiyu Song*,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Hongliang He*,</nobr>
                
              
            
          
        
          
            
              
                <nobr><em><nobr><em>Haofei</em></nobr> <nobr><em>Yu</em></nobr></em>,</nobr>
              
            
          
        
          
            
              
                
                  <nobr>Pengfei Fang,</nobr>
                
              
            
          
        
          
            
              
                
                  <nobr>Leyang Cui,</nobr>
                
              
            
          
        
          
            
              and
              
                
                  <nobr>Zhenzhong Lan</nobr>
                
              
            
          
        
      </div>

      <div>
        <p class="periodical font-italic">
          
            In Findings of ACL
          
          
            2023.
          
        </p>
      </div>
    
      <div class="col p-0">
        
          <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse" href="#song-etal-2023-uni-abstract" role="button" aria-expanded="false" aria-controls="song-etal-2023-uni-abstract">Abstract</a>
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://aclanthology.org/2023.findings-acl.388/" target="_blank">PDF</a>
        
        
        
        
        
        
          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/dll-wu/Uni-Encoder" target="_blank">Code</a>
        
        
        
        
      </div>
    
      
      <div class="col mt-2 p-0">
        <div id="song-etal-2023-uni-abstract" class="collapse">
          <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
            Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-Encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps the full attention over each pair as in Cross-Encoder while only encoding the context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates to ensure they are treated equally and design a new attention mechanism to avoid confusion. Our Uni-Encoder can simulate other ranking paradigms using different attention and response concatenation methods. Extensive experiments show that our proposed paradigm achieves new state-of-the-art results on four benchmark datasets with high computational efficiency. For instance, it improves R10@1 by 2.9% with an approximately 4X faster inference speed on the Ubuntu V2 dataset.
          </div>
        </div>
      </div>
      
    </div>
  </div>
</div>
</li></ol>
    </div>
  </div>



  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2024 Haofei Yu.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="http://localhost:4000/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="http://localhost:4000/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="http://localhost:4000/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="http://localhost:4000/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="http://localhost:4000/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="http://localhost:4000/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-105573982-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
